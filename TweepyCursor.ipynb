{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweepyCursor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOd1BxkUEDJWeqCpSqybxfQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valfeclone/ML-Layani/blob/main/TweepyCursor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvAmoQ9lklwF",
        "outputId": "e4809881-cb7a-4b35-83fd-c47cfdf25724"
      },
      "source": [
        "pip install tweepy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZbgZK_q5rSz"
      },
      "source": [
        "\n",
        "# API Tweepy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnlpWT7UdVIl"
      },
      "source": [
        "import tweepy\n",
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# Authenticate to Twitter\n",
        "auth = tweepy.OAuthHandler(\"KD8NW5hyiGCIhwp4JDJ98pgXh\", \"SQeokZiYEb5nqWiRMzwRIWF2QQyeBANeHvyfYg3BjenCp2T5vV\")\n",
        "auth.set_access_token(\"93126766-LZ166ZBBYttZ4JM5WKg9cLLBq0eoJyA0QJwq7l30V\", \"BlyNL7pReqLDQx0uWjNkMeXlg4w2j2U7cFFpPE1ATy29X\")\n",
        "api = tweepy.API(auth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frNgeNrM5uC4"
      },
      "source": [
        "Collecting Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0I2dlog2PJ6"
      },
      "source": [
        "import datetime\n",
        "\n",
        "def import_tweet():\n",
        "  cursor = tweepy.Cursor(api.search, q=[\"listrik mati\"], lang=\"id\", tweet_mode=\"extended\").items(500)\n",
        "  csvFile = open('jokowi gimana.csv', 'w')\n",
        "  csvWriter = csv.writer(csvFile)\n",
        "  csvWriter.writerow([\"Timestamp\", \"ID\", \"Username\", \"Text\"])\n",
        "  for tweet in cursor:\n",
        "    now = datetime.datetime.now()\n",
        "    remain = now - tweet.created_at\n",
        "    secs = remain.total_seconds()\n",
        "    # print(secs)\n",
        "    # print(tweet)\n",
        "    # print(secs)\n",
        "    if (secs < 7200):\n",
        "      if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
        "        # c.append(tweet.created_at + timedelta(hours=7))\n",
        "        # i.append(tweet.id)\n",
        "        # u.append(tweet.user.name)\n",
        "        # t.append(tweet.full_text)\n",
        "        # l.append(tweet.location)\n",
        "        tweets = [tweet.created_at + timedelta(hours=7),tweet.id,tweet.user.screen_name,tweet.full_text]\n",
        "        csvWriter.writerow(tweets)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4tleoKQ4J6i"
      },
      "source": [
        "\n",
        "# Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDT2PhsNs2YV",
        "outputId": "2856280c-ca87-46f0-900f-b1a8dfe55bd7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import load_model\n",
        "\n",
        "def lstm_model(new_dataset):\n",
        "  dataTweet = pd.read_csv(\"CleanDataset.csv\")\n",
        "  new_dataTweet = pd.read_csv(new_dataset)\n",
        "\n",
        "  def remove_punctuation(text):\n",
        "    remover = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(remover)\n",
        "\n",
        "  def casefolding(review):\n",
        "      review = review.lower()\n",
        "      return review\n",
        "\n",
        "  def remove_stopwords(text):\n",
        "    sw = set(stopwords.words(\"indonesian\"))\n",
        "    words = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    return \" \".join(words)\n",
        "\n",
        "  dataTweet[\"Text\"] = dataTweet.Text.map(remove_punctuation)\n",
        "  dataTweet[\"Text\"] = dataTweet.Text.map(casefolding)\n",
        "  dataTweet[\"Text\"] = dataTweet.Text.map(remove_stopwords)\n",
        "\n",
        "  new_dataTweet[\"Text\"] = new_dataTweet.Text.map(remove_punctuation)\n",
        "  new_dataTweet[\"Text\"] = new_dataTweet.Text.map(casefolding)\n",
        "  new_dataTweet[\"Text\"] = new_dataTweet.Text.map(remove_stopwords)\n",
        "\n",
        "  def words_counter(text_column):\n",
        "    count = Counter()\n",
        "    for text in text_column.values:\n",
        "      for word in text.split():\n",
        "        count[word] += 1\n",
        "    return count\n",
        "\n",
        "  total_unique = len(words_counter(dataTweet.Text))\n",
        "  new_total_unique = len(words_counter(new_dataTweet.Text))\n",
        "\n",
        "  def split_data(dataset):\n",
        "  \tdataset = dataset.sample(frac=1, random_state=42)\n",
        "  \ttrain_data, val_data = dataset[:80], dataset[80:]\n",
        "  \ttrain_tweets = train_data.Text.to_numpy()\n",
        "  \ttrain_labels = train_data.Label.to_numpy()\n",
        "  \tval_tweets = val_data.Text.to_numpy()\n",
        "  \tval_labels = val_data.Label.to_numpy()\n",
        "  \treturn train_data, train_tweets, train_labels, val_data, val_tweets, val_labels\n",
        "\n",
        "  train_data, train_tweets, train_labels, val_data, val_tweets, val_labels = split_data(dataTweet)\n",
        "  test_tweets = new_dataTweet.Text.to_numpy()\n",
        "\n",
        "  def embedding(text):\n",
        "    tokenizer = Tokenizer(num_words = total_unique)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text_sequences = tokenizer.texts_to_sequences(text)\n",
        "    return text_sequences\n",
        "\n",
        "  def padding(text_seq):\n",
        "    text_padded = pad_sequences(text_seq, maxlen=40, padding=\"post\", truncating=\"post\")\n",
        "    return text_padded\n",
        "\n",
        "  train_sequences = embedding(train_tweets)\n",
        "  train_padded = padding(train_sequences)\n",
        "  val_sequences = embedding(val_tweets)\n",
        "  val_padded = padding(val_sequences)\n",
        "  test_sequences = embedding(test_tweets)\n",
        "  test_padded = padding(test_sequences)\n",
        "\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Embedding(total_unique, 32, input_length=40))\n",
        "  model.add(layers.LSTM(64, dropout=0.1))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "  optim = keras.optimizers.Adam(learning_rate=0.001)\n",
        "  metrics = [\"accuracy\"]\n",
        "\n",
        "  model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
        "\n",
        "  model.fit(train_padded, train_labels, epochs=20, validation_data=(val_padded, val_labels), verbose=False)\n",
        "\n",
        "  predictions = model.predict(test_padded)\n",
        "  predictions = [1 if p > 0.5 else 0 for p in predictions]\n",
        "\t\n",
        "  # csvFile2 = open('prabowo gimana.csv', 'w')\n",
        "  # csvWriter2 = csv.writer(csvFile2)\n",
        "  # csvWriter2.writerow([\"Timestamp\", \"ID\", \"Username\", \"Text\"])\n",
        "  \n",
        "  complaint = []\n",
        "  new_complaints = {}\n",
        "  counter = 0\n",
        "  for i in range(len(predictions)):\n",
        "    if predictions[i] == 1:\n",
        "      complaint.append(new_dataTweet[\"Text\"][i])\n",
        "      new_complaints[counter] = {'Timestamp': new_dataTweet[\"Timestamp\"][i], \n",
        "                                 'ID': int(new_dataTweet[\"ID\"][i]), \n",
        "                                 'Username': new_dataTweet[\"Username\"][i], \n",
        "                                 'Text': new_dataTweet[\"Text\"][i]}     \n",
        "      counter += 1  \n",
        "\n",
        "  return new_complaints"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DR7u3OGs6Fj"
      },
      "source": [
        "import time\n",
        "\n",
        "try:\n",
        "  import_tweet()\n",
        "  lstm_model(\"jokowi gimana.csv\")\n",
        "\n",
        "except pd.errors.EmptyDataError:\n",
        "  time.sleep(5)\n",
        "  import_tweet()\n",
        "  lstm_model(\"jokowi gimana.csv\")\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSXbFpNv5GG7",
        "outputId": "a3081f89-debf-49fe-a8cb-244b63f8d278"
      },
      "source": [
        "pip install flask_ngrok"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask_ngrok in /usr/local/lib/python3.7/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW_dcVXP3jp2",
        "outputId": "cf23ae41-3116-4cbf-dd42-6a34d3c8d3df"
      },
      "source": [
        "from flask import Flask, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app) \n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    gatau = lstm_model(\"jokowi gimana.csv\")\n",
        "    return jsonify(gatau)\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://30026507c4d7.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [30/May/2021 21:00:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [30/May/2021 21:00:01] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3PKSwoE5DZp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}